{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Ask Sarah about how editions are separated/duplicates. then, code this up for all editions\n",
    "\n",
    "# run two_data & port re-format & visualisation from CW2\n",
    "\n",
    "\n",
    "# check that clean_up_definitions does a proper clean inplace\n",
    "# look through CW2 cleanup & code: what could be ported over\n",
    "# for_later notes eg. \\n clean\n",
    "\n",
    "# DONE\n",
    "# words/characters/volumes per edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text files: 195\n"
     ]
    }
   ],
   "source": [
    "# reading in index file\n",
    "\n",
    "inventory = pd.read_csv(\"encyclopaediaBritannica-inventory.csv\", header=None)\n",
    "inventory.columns = ['file','volume']\n",
    "# print(inventory)\n",
    "print(\"\\nNumber of text files: \" + str(len(inventory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Encyclopaedia Britannica; or, A dictionary of arts and sciences, compiled upon a new plan … - First edition, 1771, Volume 1, A-B - EB.1',\n",
       " 'Encyclopaedia Britannica; or, A dictionary of arts and sciences, compiled upon a new plan … - First edition, 1771, Volume 2, C-L - EB.1',\n",
       " 'Encyclopaedia Britannica; or, A dictionary of arts and sciences, compiled upon a new plan … - First edition, 1771, Volume 3, M-Z - EB.1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ed = inventory[:3]\n",
    "list(first_ed['volume']) # list() to print whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Encyclopaedia Britannica - Second edition, Volume 1, A-AST - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 2, Astronomy-BZO - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 3, C - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 4, D-F - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 5, G-J - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 6, K-Medicine - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 7, Medicines-Optics - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 8, Optics-Poetry - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 9, POI-SCU - EB.4',\n",
       " 'Encyclopaedia Britannica - Second edition, Volume 10, SCU-Appendix - EB.4']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_ed = inventory[6:16]\n",
    "list(second_ed['volume']) # list() to print whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_editions = [first_ed, second_ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in text files and generate volume, word and length counts\n",
    "\n",
    "def basic_data(editions):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for ed in editions: \n",
    "        length = 0\n",
    "        words = 0\n",
    "        volumes = 0\n",
    "\n",
    "        for index, row in ed.iterrows():\n",
    "            print(\"Reading: \" + row['file'])\n",
    "\n",
    "            f = open('text/' + row['file'], 'r', encoding=\"utf8\")\n",
    "            content = f.read()\n",
    "            length += len(content)\n",
    "            volumes += 1\n",
    "\n",
    "            words += len(content.split())\n",
    "\n",
    "            f.close()\n",
    "            \n",
    "        data.append([volumes, length, words])\n",
    "        data_df=pd.DataFrame(data,columns=['volumes', 'length_chars','length_words'])\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 144133901.txt\n",
      "Reading: 144133902.txt\n",
      "Reading: 144133903.txt\n",
      "Reading: 144850370.txt\n",
      "Reading: 144850373.txt\n",
      "Reading: 144850374.txt\n",
      "Reading: 144850375.txt\n",
      "Reading: 144850376.txt\n",
      "Reading: 144850377.txt\n",
      "Reading: 144850378.txt\n",
      "Reading: 144850379.txt\n",
      "Reading: 190273289.txt\n",
      "Reading: 190273290.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volumes</th>\n",
       "      <th>length_chars</th>\n",
       "      <th>length_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>14223433</td>\n",
       "      <td>2567769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>56865594</td>\n",
       "      <td>10173856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   volumes  length_chars  length_words\n",
       "0        3      14223433       2567769\n",
       "1       10      56865594      10173856"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_data(two_editions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most referenced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_by(s, a, b):\n",
    "    \"\"\" perform a regex replacement, prints number of occurrences found and returns a string.\n",
    "    s: string to make changes in\n",
    "    a: string to remove \n",
    "    b: string to add\n",
    "    returns: a string\"\"\"\n",
    "    \n",
    "    # print(\"Replacing \\\"\" + a + \"\\\" by \\\"\" + b + \"\\\", found \" + str(len(re.findall(a, s))) + \"...\")\n",
    "    new = re.sub(a, b, s)\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using regex cleanup ideas from: https://sites.temple.edu/tudsc/2014/08/12/text-scrubbing-hacks-cleaning-your-ocred-text/\n",
    "\n",
    "# Further resources: https://programminghistorian.org/en/lessons/cleaning-ocrd-text-with-regular-expressions\n",
    "# https://datascience.stackexchange.com/questions/20536/how-to-improve-ocr-scanning-results\n",
    "\n",
    "def clean_up(s):\n",
    "    \"\"\" Does a minimal cleanup of a string of text\n",
    "    returns: a string\n",
    "    \"\"\"\n",
    "    # print(\"Initial length: \" + str(len(s)))\n",
    "    s2 = replace_by(s, 'tbe', 'the')\n",
    "    s3 = replace_by(s2, 'tiie', 'the')\n",
    "    s4 = replace_by(s3, 'liis', 'his')\n",
    "    s5 = replace_by(s4, 'bis', 'his')\n",
    "    s6 = replace_by(s5, '■', '')\n",
    "    s7 = replace_by(s6, 'ib','in') # was noticed by Ava in .txt files\n",
    "    s8 = replace_by(s7, '¬','')\n",
    "    s9 = replace_by(s8, '.(\\.\\.+)', '') # multiple periods; what about ellipsis though?\n",
    "    # print(\"Clean up done!\")\n",
    "    return s9 # CAREFUL to always pass the right one to next, and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses the list of header/definition/refs dataframes for each edition\n",
    "# Check that this doesn't just do a reference replace, actually changes editions that are passed to it\n",
    "\n",
    "def clean_up_definitions(editions):\n",
    "    editions_cleaned = []\n",
    "    for ed in editions:\n",
    "        # realistically, a definition can't be shorter than 5 letters, even if it is simply a redirection of the form of \"See x\"\n",
    "        ed = ed.loc[ed['def_length'] > 5]\n",
    "        # remove headers with two letters as they are probably mostly noise\n",
    "        ed = ed.loc[ed['header_length'] > 2]\n",
    "        # remove headers longer than 40 characters, as these are likely to be noise\n",
    "        ed = ed.loc[ed['header_length'] < 40]\n",
    "        \n",
    "        # We might also want to drop header duplicates, as each entry is only defined once\n",
    "        # A good portion of these duplicates are probably noise (such as annotations in roman numerals)\n",
    "        # We're not selecting the correct definition out of the duplicates here, just dropping them for the sake of simplicity\n",
    "        ed.drop_duplicates(subset =\"headers\", keep = False, inplace = True)\n",
    "        \n",
    "        editions_cleaned.append(ed)\n",
    "        \n",
    "    return editions_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For extracting words and their definitions, and references (\"See x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses RegEx to identify the definition of a word as being the content between the uppercase word and the next uppercase word\n",
    "\n",
    "def find_definition(word, volume):\n",
    "    start = re.search(word, volume).start()\n",
    "    start_next = re.search(word, volume).end()\n",
    "    \n",
    "    second_word = re.search(\"[A-Z][A-Z]+\", volume[start_next:])\n",
    "    \n",
    "    if second_word: # checking that second capitalised word exists (prevents NoneType exception)\n",
    "        end = re.search(\"[A-Z][A-Z]+\", volume[start_next:]).start()\n",
    "        return volume[start_next+2:start_next+end] # +2 to ignore comma and space before a definition\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts entries (headers), their definitions, their respective lengths, and all references (\"See x\") from an edition\n",
    "\n",
    "def extract_info(edition):\n",
    "    \n",
    "    headers = []\n",
    "    definitions = []\n",
    "    refs = [] # for counting up all instances of \"See x\" in volumes  \n",
    "    \n",
    "    for index, row in edition.iterrows():\n",
    "        current_headers = []\n",
    "        \n",
    "        print(\"Reading: \" + row['file'])\n",
    "        f = open('text/' + row['file'], 'r', encoding=\"utf8\")\n",
    "        content = f.read()\n",
    "        content = clean_up(content) # check cleanup does something\n",
    "        # using https://stackoverflow.com/questions/9525993/get-consecutive-capitalized-words-using-regex\n",
    "        # note: this ReGex is not entirely correct, as the first will select \"MARTIAL\" and \"LAW\"\n",
    "        # \"MARTIAL\" will be cleaned out later due to having an empty definition, but \"LAW\" will have duplicates\n",
    "        \n",
    "        current_headers = re.findall('[A-Z][A-Z]+', content) # at least two uppercase letters following each other\n",
    "        current_headers += re.findall('([A-Z][A-Z]+(?=\\s[A-Z])(?:\\s[A-Z][A-Z]+)+)', content) # two or more uppercase words\n",
    "        refs += re.findall('See [^,\\.]*', content) # matches \"See x\" until a comma or a period\n",
    "        \n",
    "        for word in current_headers:\n",
    "            definitions.append(find_definition(word, content))\n",
    "            \n",
    "        headers = headers + current_headers\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "    data = pd.DataFrame(headers, columns =['headers'])\n",
    "    data['definition'] = definitions\n",
    "    data['header_length']  = data['headers'].str.len()\n",
    "    data['def_length']  = data['definition'].str.len()\n",
    "    return data, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a list of dataframes with words and their definitions for each edition\n",
    "\n",
    "def words_and_definitions(editions):\n",
    "    \n",
    "    all_edition_data = []\n",
    "    all_references = []\n",
    "    \n",
    "    for ed in editions:\n",
    "        edition_data, references = extract_info(ed)\n",
    "        all_edition_data.append(edition_data)\n",
    "        all_references.append(references)\n",
    "    return all_edition_data, all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 144133901.txt\n",
      "Reading: 144133902.txt\n",
      "Reading: 144133903.txt\n",
      "Reading: 144850370.txt\n",
      "Reading: 144850373.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-f07fa8fc0eed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtwo_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwo_refs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_and_definitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwo_editions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-1975ea3d4816>\u001b[0m in \u001b[0;36mwords_and_definitions\u001b[1;34m(editions)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0med\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meditions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0medition_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreferences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0med\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mall_edition_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medition_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mall_references\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-943ef147d854>\u001b[0m in \u001b[0;36mextract_info\u001b[1;34m(edition)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcurrent_headers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mdefinitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_definition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurrent_headers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-56ba3e143b8a>\u001b[0m in \u001b[0;36mfind_definition\u001b[1;34m(word, volume)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolume\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstart_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolume\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[A-Z][A-Z]+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolume\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_next\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvolume\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_next\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstart_next\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# +2 to ignore comma and space before a definition\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "two_data, two_refs = words_and_definitions(two_editions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 144850370.txt\n",
      "Reading: 144850373.txt\n",
      "Reading: 144850374.txt\n",
      "Reading: 144850375.txt\n",
      "Reading: 144850376.txt\n",
      "Reading: 144850377.txt\n",
      "Reading: 144850378.txt\n",
      "Reading: 144850379.txt\n",
      "Reading: 190273289.txt\n",
      "Reading: 190273290.txt\n"
     ]
    }
   ],
   "source": [
    "second_data, refs = extract_info(second_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_data_cleaned = clean_up_definitions([second_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headers</th>\n",
       "      <th>definition</th>\n",
       "      <th>header_length</th>\n",
       "      <th>def_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>AACH</td>\n",
       "      <td>a little town in Germany, in the circle of\\nSu...</td>\n",
       "      <td>4</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>AAHUS</td>\n",
       "      <td>a little town in Germany, in the circle of\\nWe...</td>\n",
       "      <td>5</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>AARON</td>\n",
       "      <td>high-prieft of the Jews, and brother to\\nMofes...</td>\n",
       "      <td>5</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>AARSENS</td>\n",
       "      <td>Peter), a painter, called in Italy Pietro\\nEon...</td>\n",
       "      <td>7</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>ABACATUAIA</td>\n",
       "      <td>in ichthyology, a barbarous\\nname of the zeus ...</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58499</th>\n",
       "      <td>WAR\\nWAR</td>\n",
       "      <td>9184 ]\\nw.\\nSee Virgula Divina in this\\n</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58500</th>\n",
       "      <td>AX AX</td>\n",
       "      <td>) uinquefolium.\\n</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58501</th>\n",
       "      <td>WIN TERANTA</td>\n",
       "      <td>romatic a. Tlate</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58504</th>\n",
       "      <td>CCC IV</td>\n",
       "      <td>)\\ncccv. C\\ncccvio\\n</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58505</th>\n",
       "      <td>CCC IX</td>\n",
       "      <td>cccx.\\n</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19121 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           headers                                         definition  \\\n",
       "111           AACH  a little town in Germany, in the circle of\\nSu...   \n",
       "112          AAHUS  a little town in Germany, in the circle of\\nWe...   \n",
       "115          AARON  high-prieft of the Jews, and brother to\\nMofes...   \n",
       "118        AARSENS  Peter), a painter, called in Italy Pietro\\nEon...   \n",
       "122     ABACATUAIA  in ichthyology, a barbarous\\nname of the zeus ...   \n",
       "...            ...                                                ...   \n",
       "58499     WAR\\nWAR           9184 ]\\nw.\\nSee Virgula Divina in this\\n   \n",
       "58500        AX AX                                  ) uinquefolium.\\n   \n",
       "58501  WIN TERANTA                                  romatic a. Tlate    \n",
       "58504       CCC IV                               )\\ncccv. C\\ncccvio\\n   \n",
       "58505       CCC IX                                            cccx.\\n   \n",
       "\n",
       "       header_length  def_length  \n",
       "111                4         309  \n",
       "112                5         209  \n",
       "115                5         857  \n",
       "118                7         402  \n",
       "122               10          62  \n",
       "...              ...         ...  \n",
       "58499              7          38  \n",
       "58500              5          16  \n",
       "58501             11          17  \n",
       "58504              6          17  \n",
       "58505              6           6  \n",
       "\n",
       "[19121 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_data_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
